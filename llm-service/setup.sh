#!/bin/bash

# Lag-Llama Service Setup Script
# Sets up Python environment and installs dependencies

set -e

echo "ğŸ¦™ Setting up Lag-Llama Service..."

# Check Python version
if ! command -v python3 &> /dev/null; then
    echo "âŒ Python 3 is required but not installed."
    exit 1
fi

PYTHON_VERSION=$(python3 --version | cut -d' ' -f2 | cut -d'.' -f1,2)
echo "âœ… Python version: $PYTHON_VERSION"

# Create virtual environment
if [ ! -d "venv" ]; then
    echo "ğŸ“¦ Creating virtual environment..."
    python3 -m venv venv
else
    echo "âœ… Virtual environment already exists"
fi

# Activate virtual environment
source venv/bin/activate

# Upgrade pip
echo "â¬†ï¸  Upgrading pip..."
pip install --upgrade pip

# Install dependencies
echo "ğŸ“¥ Installing dependencies..."
pip install -r requirements.txt

# Download Lag-Llama model (first time only)
echo "ğŸ”½ Downloading Lag-Llama model..."
python3 << 'EOF'
from transformers import AutoModelForCausalLM, AutoTokenizer
import os

model_name = "time-series-foundation-models/lag-llama"
cache_dir = os.path.expanduser("~/.cache/huggingface")

print(f"Downloading model to: {cache_dir}")

try:
    # This will download and cache the model
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        trust_remote_code=True,
        torch_dtype="auto"
    )
    print("âœ… Model downloaded successfully!")
except Exception as e:
    print(f"âš ï¸  Model download failed (will retry on first API call): {e}")
EOF

# Create .env if not exists
if [ ! -f ".env" ]; then
    echo "ğŸ“ Creating .env file..."
    cp .env.example .env
fi

echo ""
echo "âœ… Setup complete!"
echo ""
echo "To start the service:"
echo "  source venv/bin/activate"
echo "  python app.py"
echo ""
echo "Or use the start script:"
echo "  ./start.sh"





