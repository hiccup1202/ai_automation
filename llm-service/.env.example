# Lag-Llama Service Configuration

# Service Port
LLM_SERVICE_PORT=8000

# Model Configuration
LAG_LLAMA_MODEL_PATH=time-series-foundation-models/lag-llama

# Inference Settings
MAX_CONTEXT_LENGTH=64
DEFAULT_PREDICTION_LENGTH=7
NUM_SAMPLES=100

# Device (cuda or cpu)
DEVICE=cpu
